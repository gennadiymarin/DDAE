{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/gm_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as tt\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from diffusers import DDPMPipeline\n",
    "\n",
    "DEVICE = 'cuda'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_deterministic(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def readable_number(num):\n",
    "    num_str = str(num)[::-1]\n",
    "    res = ''\n",
    "    i_prev = 0\n",
    "    for i in range(3, len(num_str), 3):\n",
    "        res += num_str[i_prev:i] + ','\n",
    "        i_prev = i\n",
    "    if i_prev < len(num_str):\n",
    "        res += num_str[i_prev:]\n",
    "    return res[::-1]\n",
    "\n",
    "def log(writer, metrics, epoch):\n",
    "    writer.add_scalars('loss', {'train': metrics['loss_train'], 'test': metrics['loss_test']}, epoch)\n",
    "    writer.add_scalars('accuracy', {'train': metrics['accuracy_train'], 'test': metrics['accuracy_test']}, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "def save_checkpoint(state, path, epoch, test_loss):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(state, f'{path}/{epoch}_valloss={test_loss:.3f}.pt')\n",
    "\n",
    "def get_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return total, trainable\n",
    "\n",
    "def print_parameters(model):\n",
    "    total, trainable = get_parameters(model)\n",
    "    print(f'model initialized with trainable params: {readable_number(trainable)} || total params: {readable_number(total)} || trainable%: {trainable/total * 100:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fp16(model, criterion, data_loader, tqdm_desc, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred, test_loss = [], [], []\n",
    "    for imgs, target in data_loader:\n",
    "    # for imgs, target in tqdm(data_loader, desc=tqdm_desc):\n",
    "        imgs, target = imgs.to(device), target.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                logits = model(imgs)\n",
    "                loss = criterion(logits, target)\n",
    "\n",
    "        test_loss.append(loss.item())\n",
    "        y_pred.extend(logits.argmax(dim=1).flatten().tolist())\n",
    "        y_true.extend(target.flatten().tolist())\n",
    "\n",
    "    y_true, y_pred, test_loss = np.array(y_true), np.array(y_pred), np.array(test_loss)\n",
    "    metrics = {}\n",
    "    metrics['accuracy_test'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['loss_test'] = np.mean(test_loss)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_fp16_epoch(model, optimizer, criterion, scheduler, data_loader, tqdm_desc, scaler, device):\n",
    "    model.train()\n",
    "    y_true, y_pred, train_loss = [], [], [] \n",
    "    for imgs, target in data_loader:\n",
    "    # for imgs, target in tqdm(data_loader, desc=tqdm_desc):\n",
    "        imgs, target = imgs.to(device), target.to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        y_pred.extend(logits.argmax(dim=1).flatten().tolist())\n",
    "        y_true.extend(target.flatten().tolist())\n",
    "\n",
    "    y_true, y_pred, train_loss = np.array(y_true), np.array(y_pred), np.array(train_loss)\n",
    "    metrics = {}\n",
    "    metrics['accuracy_train'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['loss_train'] = np.mean(train_loss)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_fp16(writer, model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, freq_save, save_path, scaler, device):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start = time.time()\n",
    "        metrics_train = train_fp16_epoch(\n",
    "            model, optimizer, criterion, scheduler, train_loader,\n",
    "            tqdm_desc=f'Training {epoch}/{num_epochs}', scaler=scaler, device=device\n",
    "        )\n",
    "        metrics_val = test_fp16(\n",
    "            model, criterion, val_loader,\n",
    "            tqdm_desc=f'Validating {epoch}/{num_epochs}', device=device\n",
    "        )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % freq_save == 0:\n",
    "            save_checkpoint(model.state_dict(), save_path, epoch, metrics_val[\"loss_test\"])\n",
    "          \n",
    "        log(writer, {**metrics_val, **metrics_train}, epoch)\n",
    "        end = time.time()\n",
    "        print(f'{epoch=} in {((end - start) / 60):.2f}m, loss_val={metrics_val[\"loss_test\"]:.3f}, loss_train={metrics_train[\"loss_train\"]:.3f}, acc_val={metrics_val[\"accuracy_test\"]:.3f}, acc_train={metrics_train[\"accuracy_train\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  7.07it/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\")\n",
    "# pipe = DDPMPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionEncoder(nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "    \n",
    "    def forward(self, imgs, timestep, class_labels=None, up_last=-1, GAP=True):\n",
    "        params = 0\n",
    "        # 0. center input if necessary\n",
    "        if self.unet.config.center_input_sample:\n",
    "            imgs = 2 * imgs - 1.0\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=imgs.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(imgs.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(imgs.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "\n",
    "        t_emb = self.unet.time_proj(timesteps)\n",
    "\n",
    "        # timesteps does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=self.unet.dtype)\n",
    "        emb = self.unet.time_embedding(t_emb)\n",
    "\n",
    "        total = get_parameters(self.unet.time_embedding)[0]\n",
    "        params += total\n",
    "        # print(f'time_embedding {total}')\n",
    "\n",
    "        if self.unet.class_embedding is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\"class_labels should be provided when doing class conditioning\")\n",
    "\n",
    "            if self.unet.config.class_embed_type == \"timestep\":\n",
    "                class_labels = self.unet.time_proj(class_labels)\n",
    "\n",
    "            class_emb = self.unet.class_embedding(class_labels).to(dtype=self.unet.dtype)\n",
    "            emb = emb + class_emb\n",
    "\n",
    "            total = get_parameters(self.unet.class_embedding)[0]\n",
    "            params += total\n",
    "            # print(f'time_embedding {total}')\n",
    "        elif self.unet.class_embedding is None and class_labels is not None:\n",
    "            raise ValueError(\"class_embedding needs to be initialized in order to use class conditioning\")\n",
    "\n",
    "        # 2. pre-process\n",
    "        skip_sample = imgs\n",
    "        imgs = self.unet.conv_in(imgs)\n",
    "        \n",
    "        total = get_parameters(self.unet.conv_in)[0]\n",
    "        params += total\n",
    "        # print(f'conv_in {total}')\n",
    "        \n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (imgs,)\n",
    "        for downsample_block in self.unet.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                imgs, res_samples, skip_sample = downsample_block(\n",
    "                    hidden_states=imgs, temb=emb, skip_sample=skip_sample\n",
    "                )\n",
    "            else:\n",
    "                imgs, res_samples = downsample_block(hidden_states=imgs, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "            total = get_parameters(downsample_block)[0]\n",
    "            params += total\n",
    "            # print(f'downsample_block {total}')\n",
    "\n",
    "        # 4. mid\n",
    "        imgs = self.unet.mid_block(imgs, emb)\n",
    "        # print(f'midlle, {imgs.shape=}')\n",
    "\n",
    "        total = get_parameters(self.unet.mid_block)[0]\n",
    "        params += total\n",
    "        # print(f'mid_block {total}')\n",
    "\n",
    "        # 5. up\n",
    "        skip_sample = None\n",
    "        for i, upsample_block in enumerate(self.unet.up_blocks):\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            if hasattr(upsample_block, \"skip_conv\"):\n",
    "                imgs, skip_sample = upsample_block(imgs, res_samples, emb, skip_sample)\n",
    "            else:\n",
    "                imgs = upsample_block(imgs, res_samples, emb)\n",
    "\n",
    "            total = get_parameters(upsample_block)[0]\n",
    "            params += total\n",
    "            # print(f'upsample_block {total}')\n",
    "\n",
    "            if up_last == i:\n",
    "                print(f'params used = {readable_number(params)}')\n",
    "                return imgs.mean(dim=[2, 3]) if GAP else imgs\n",
    "        \n",
    "            \n",
    "        # 6. post-process\n",
    "        imgs = self.unet.conv_norm_out(imgs)\n",
    "        imgs = self.unet.conv_act(imgs)\n",
    "        imgs = self.unet.conv_out(imgs)\n",
    "\n",
    "        if skip_sample is not None:\n",
    "            imgs += skip_sample\n",
    "\n",
    "        if self.unet.config.time_embedding_type == \"fourier\":\n",
    "            timesteps = timesteps.reshape((imgs.shape[0], *([1] * len(imgs.shape[1:]))))\n",
    "            imgs = imgs / timesteps\n",
    "\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params used = 103,741,440\n",
      "model initialized with trainable params: 113,673,219 || total params: 113,673,219 || trainable%: 100.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 64, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img, t = torch.randn(3, 3, 32, 32).to(DEVICE), torch.tensor([0]).to(DEVICE)\n",
    "img, t = torch.randn(3, 3, 256, 256).to(DEVICE), torch.tensor([0]).to(DEVICE)\n",
    "encoder = DiffusionEncoder(hf_unet).to(DEVICE)\n",
    "\n",
    "out = encoder(img, t, up_last=2, GAP=False)\n",
    "print_parameters(encoder)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(torch.all(hf_unet(img, t, return_dict=False)[0] == encoder(img, t)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(128, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=self.scheduler.config.num_train_timesteps, size=[x.shape[0]]).to(x.device)\n",
    "        noise = torch.randn(x.shape).to(x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "\n",
    "img, t = torch.randn(3, 3, 32, 32).to(DEVICE), torch.tensor([0]).to(DEVICE)\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "classifier = ClassifierNoised(backbone, hf_scheduler).to(DEVICE)\n",
    "out = classifier(img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, backbone, up_last=3, t=0):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(128, 100)\n",
    "        self.up_last = up_last\n",
    "        self.t = t\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x, self.t, up_last=self.up_last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, t = torch.randn(3, 3, 32, 32).to(DEVICE), torch.tensor([0]).to(DEVICE)\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "classifier = Classifier(backbone).to(DEVICE)\n",
    "\n",
    "out = classifier(img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size, dataset='cifar100', horizontal_flip=False):\n",
    "    transform_test = tt.Compose([tt.ToTensor(), tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225])])\n",
    "    transform_train = tt.Compose([tt.ToTensor(), tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225]), tt.RandomHorizontalFlip()]) if horizontal_flip else transform_test\n",
    "    \n",
    "    if dataset == 'cifar100':\n",
    "        dataset_train = torchvision.datasets.CIFAR100(root='./datasets/cifar100', train=True, download=True, transform=transform_train)\n",
    "        dataset_test = torchvision.datasets.CIFAR100(root='./datasets/cifar100', train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    if dataset == 'cifar10':\n",
    "        dataset_train = torchvision.datasets.CIFAR10(root='./datasets/cifar10', train=True, download=True, transform=transform_train)\n",
    "        dataset_test = torchvision.datasets.CIFAR10(root='./datasets/cifar10', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=4)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=512\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.61m, loss_val=2.067, loss_train=2.756, acc_val=0.454, acc_train=0.343\n",
      "epoch=2 in 0.61m, loss_val=1.736, loss_train=1.936, acc_val=0.516, acc_train=0.478\n",
      "epoch=3 in 0.61m, loss_val=1.653, loss_train=1.683, acc_val=0.550, acc_train=0.535\n",
      "epoch=4 in 0.62m, loss_val=1.474, loss_train=1.513, acc_val=0.588, acc_train=0.574\n",
      "epoch=5 in 0.60m, loss_val=1.429, loss_train=1.357, acc_val=0.602, acc_train=0.612\n",
      "epoch=6 in 0.59m, loss_val=1.329, loss_train=1.214, acc_val=0.630, acc_train=0.649\n",
      "epoch=7 in 0.59m, loss_val=1.275, loss_train=1.060, acc_val=0.656, acc_train=0.689\n",
      "epoch=8 in 0.62m, loss_val=1.221, loss_train=0.924, acc_val=0.667, acc_train=0.726\n",
      "epoch=9 in 0.59m, loss_val=1.186, loss_train=0.773, acc_val=0.685, acc_train=0.762\n",
      "epoch=10 in 0.60m, loss_val=1.102, loss_train=0.633, acc_val=0.695, acc_train=0.801\n",
      "epoch=11 in 0.60m, loss_val=1.050, loss_train=0.494, acc_val=0.722, acc_train=0.844\n",
      "epoch=12 in 0.59m, loss_val=1.044, loss_train=0.389, acc_val=0.729, acc_train=0.875\n",
      "epoch=13 in 0.61m, loss_val=1.030, loss_train=0.295, acc_val=0.739, acc_train=0.905\n",
      "epoch=14 in 0.62m, loss_val=1.013, loss_train=0.241, acc_val=0.745, acc_train=0.924\n",
      "epoch=15 in 0.62m, loss_val=1.002, loss_train=0.209, acc_val=0.750, acc_train=0.935\n"
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(256, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=50, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'noised_cls_cifar100_uplast1'\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 15\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar100', horizontal_flip=True)\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"DanielBairamian/ddpm-cifar10-32-ema\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]\n",
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--google--ddpm-ema-cat-256/snapshots/9517646c0efb301d44709f2e294f1548a6fdc408: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--google--ddpm-ema-cat-256/snapshots/9517646c0efb301d44709f2e294f1548a6fdc408.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "pipe = DDPMPipeline.from_pretrained(\"google/ddpm-ema-cat-256\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--google--ddpm-ema-cat-256/snapshots/9517646c0efb301d44709f2e294f1548a6fdc408: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--google--ddpm-ema-cat-256/snapshots/9517646c0efb301d44709f2e294f1548a6fdc408.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  2.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m scaler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mGradScaler()\n\u001b[1;32m     44\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tensorboard/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mtrain_fp16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFREQ_SAVE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./checkpoints/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m, in \u001b[0;36mtrain_fp16\u001b[0;34m(writer, model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, freq_save, save_path, scaler, device)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     53\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 54\u001b[0m     metrics_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fp16_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     metrics_val \u001b[38;5;241m=\u001b[39m test_fp16(\n\u001b[1;32m     59\u001b[0m         model, criterion, val_loader,\n\u001b[1;32m     60\u001b[0m         tqdm_desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mtrain_fp16_epoch\u001b[0;34m(model, optimizer, criterion, scheduler, data_loader, tqdm_desc, scaler, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, target)\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     38\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(512, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=50, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'noised_cls_cifar100_256'\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 15\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "transform_test = tt.Compose([tt.ToTensor(), tt.Resize(256), tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225])])\n",
    "transform_train = tt.Compose([tt.ToTensor(),  tt.Resize(256), tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225]), tt.RandomHorizontalFlip()])\n",
    "\n",
    "dataset_train = torchvision.datasets.CIFAR100(root='./datasets/cifar100', train=True, download=True, transform=transform_train)\n",
    "dataset_test = torchvision.datasets.CIFAR100(root='./datasets/cifar100', train=False, download=True, transform=transform_test)\n",
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n",
    "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, num_workers=4)\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"google/ddpm-ema-cat-256\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:08<00:00, 21097557.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/cifar10/cifar-10-python.tar.gz to ./datasets/cifar10\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1/10: 100%|██████████| 97/97 [00:57<00:00,  1.70it/s]\n",
      "Validating 1/10: 100%|██████████| 19/19 [00:06<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 1.06m, loss_val=0.469, loss_train=4.304, acc_val=0.848, acc_train=0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 97/97 [00:57<00:00,  1.67it/s]\n",
      "Validating 2/10: 100%|██████████| 19/19 [00:05<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 1.06m, loss_val=0.385, loss_train=0.300, acc_val=0.874, acc_train=0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 97/97 [00:57<00:00,  1.68it/s]\n",
      "Validating 3/10: 100%|██████████| 19/19 [00:06<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 1.06m, loss_val=0.349, loss_train=0.185, acc_val=0.892, acc_train=0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 97/97 [00:57<00:00,  1.69it/s]\n",
      "Validating 4/10: 100%|██████████| 19/19 [00:05<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 1.06m, loss_val=0.352, loss_train=0.117, acc_val=0.895, acc_train=0.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 97/97 [00:57<00:00,  1.69it/s]\n",
      "Validating 5/10: 100%|██████████| 19/19 [00:05<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 1.05m, loss_val=0.367, loss_train=0.071, acc_val=0.901, acc_train=0.978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 97/97 [00:56<00:00,  1.71it/s]\n",
      "Validating 6/10: 100%|██████████| 19/19 [00:05<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 1.04m, loss_val=0.379, loss_train=0.037, acc_val=0.899, acc_train=0.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 97/97 [00:57<00:00,  1.68it/s]\n",
      "Validating 7/10: 100%|██████████| 19/19 [00:06<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 1.06m, loss_val=0.401, loss_train=0.021, acc_val=0.903, acc_train=0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 97/97 [00:56<00:00,  1.70it/s]\n",
      "Validating 8/10: 100%|██████████| 19/19 [00:05<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 1.05m, loss_val=0.403, loss_train=0.013, acc_val=0.906, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 97/97 [00:56<00:00,  1.71it/s]\n",
      "Validating 9/10: 100%|██████████| 19/19 [00:05<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 1.04m, loss_val=0.406, loss_train=0.007, acc_val=0.906, acc_train=0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 97/97 [00:56<00:00,  1.71it/s]\n",
      "Validating 10/10: 100%|██████████| 19/19 [00:05<00:00,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 1.04m, loss_val=0.407, loss_train=0.005, acc_val=0.908, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, backbone, up_last=3, t=0):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(128, 100)\n",
    "        self.up_last = up_last\n",
    "        self.t = t\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x, self.t, up_last=self.up_last))\n",
    "\n",
    "\n",
    "NAME = 'cls_cifar10_base'\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "transform_test = tt.Compose([tt.ToTensor(), tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225])])\n",
    "transform_train = tt.Compose([tt.ToTensor(), tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225]), tt.RandomHorizontalFlip()])\n",
    "\n",
    "dataset_train = torchvision.datasets.CIFAR10(root='./datasets/cifar10', train=True, download=True, transform=transform_train)\n",
    "dataset_test = torchvision.datasets.CIFAR10(root='./datasets/cifar10', train=False, download=True, transform=transform_test)\n",
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n",
    "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, num_workers=4)\n",
    "\n",
    "\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = Classifier(backbone).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1/10: 100%|██████████| 97/97 [00:24<00:00,  3.98it/s]\n",
      "Validating 1/10: 100%|██████████| 19/19 [00:03<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.47m, loss_val=0.543, loss_train=1.119, acc_val=0.888, acc_train=0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 97/97 [00:25<00:00,  3.86it/s]\n",
      "Validating 2/10: 100%|██████████| 19/19 [00:03<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.48m, loss_val=0.511, loss_train=0.068, acc_val=0.895, acc_train=0.979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 97/97 [00:24<00:00,  3.88it/s]\n",
      "Validating 3/10: 100%|██████████| 19/19 [00:03<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.47m, loss_val=0.609, loss_train=0.043, acc_val=0.889, acc_train=0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 97/97 [00:25<00:00,  3.82it/s]\n",
      "Validating 4/10: 100%|██████████| 19/19 [00:03<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.48m, loss_val=0.575, loss_train=0.028, acc_val=0.897, acc_train=0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 97/97 [00:25<00:00,  3.82it/s]\n",
      "Validating 5/10: 100%|██████████| 19/19 [00:03<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.48m, loss_val=0.573, loss_train=0.019, acc_val=0.903, acc_train=0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 97/97 [00:25<00:00,  3.84it/s]\n",
      "Validating 6/10: 100%|██████████| 19/19 [00:03<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.48m, loss_val=0.610, loss_train=0.011, acc_val=0.902, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 97/97 [00:25<00:00,  3.81it/s]\n",
      "Validating 7/10: 100%|██████████| 19/19 [00:03<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.48m, loss_val=0.591, loss_train=0.005, acc_val=0.907, acc_train=0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 97/97 [00:25<00:00,  3.84it/s]\n",
      "Validating 8/10: 100%|██████████| 19/19 [00:03<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.48m, loss_val=0.603, loss_train=0.003, acc_val=0.904, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 97/97 [00:24<00:00,  3.89it/s]\n",
      "Validating 9/10: 100%|██████████| 19/19 [00:03<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.47m, loss_val=0.602, loss_train=0.002, acc_val=0.905, acc_train=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 97/97 [00:25<00:00,  3.78it/s]\n",
      "Validating 10/10: 100%|██████████| 19/19 [00:03<00:00,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.48m, loss_val=0.600, loss_train=0.001, acc_val=0.906, acc_train=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, backbone, up_last=3, t=0):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(256, 10)\n",
    "        self.up_last = up_last\n",
    "        self.t = t\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x, self.t, up_last=self.up_last))\n",
    "\n",
    "\n",
    "NAME = 'cls_cifar10_uplast1'\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = Classifier(backbone, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1/10: 100%|██████████| 97/97 [00:28<00:00,  3.37it/s]\n",
      "Validating 1/10: 100%|██████████| 19/19 [00:03<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.54m, loss_val=37.036, loss_train=32.765, acc_val=0.868, acc_train=0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 97/97 [00:28<00:00,  3.44it/s]\n",
      "Validating 2/10: 100%|██████████| 19/19 [00:03<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.53m, loss_val=19.870, loss_train=5.051, acc_val=0.882, acc_train=0.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 97/97 [00:28<00:00,  3.45it/s]\n",
      "Validating 3/10: 100%|██████████| 19/19 [00:03<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.53m, loss_val=14.239, loss_train=2.042, acc_val=0.892, acc_train=0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 97/97 [00:28<00:00,  3.40it/s]\n",
      "Validating 4/10: 100%|██████████| 19/19 [00:03<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.53m, loss_val=11.739, loss_train=1.082, acc_val=0.899, acc_train=0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 97/97 [00:27<00:00,  3.46it/s]\n",
      "Validating 5/10: 100%|██████████| 19/19 [00:03<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.52m, loss_val=11.051, loss_train=0.502, acc_val=0.900, acc_train=0.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 97/97 [00:27<00:00,  3.47it/s]\n",
      "Validating 6/10: 100%|██████████| 19/19 [00:03<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.52m, loss_val=11.450, loss_train=0.269, acc_val=0.900, acc_train=0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 97/97 [00:28<00:00,  3.44it/s]\n",
      "Validating 7/10: 100%|██████████| 19/19 [00:03<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.52m, loss_val=10.706, loss_train=0.153, acc_val=0.901, acc_train=0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 97/97 [00:27<00:00,  3.50it/s]\n",
      "Validating 8/10: 100%|██████████| 19/19 [00:03<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.53m, loss_val=10.546, loss_train=0.068, acc_val=0.905, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 97/97 [00:28<00:00,  3.46it/s]\n",
      "Validating 9/10: 100%|██████████| 19/19 [00:03<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.53m, loss_val=10.540, loss_train=0.024, acc_val=0.906, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 97/97 [00:28<00:00,  3.41it/s]\n",
      "Validating 10/10: 100%|██████████| 19/19 [00:03<00:00,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.54m, loss_val=10.520, loss_train=0.017, acc_val=0.907, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, backbone, up_last=3, t=0, GAP=True):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.proj = nn.Conv2d(256, 256, 16)\n",
    "        self.head = nn.Linear(256, 10)\n",
    "\n",
    "        self.up_last = up_last\n",
    "        self.t = t\n",
    "        self.GAP = GAP\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x, self.t, up_last=self.up_last, GAP=self.GAP)\n",
    "        out = self.proj(out)\n",
    "        return self.head(out.squeeze())\n",
    "\n",
    "\n",
    "NAME = 'cls_cifar10_gap'\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = Classifier(backbone, up_last=1, GAP=False).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1/10: 100%|██████████| 97/97 [00:21<00:00,  4.55it/s]\n",
      "Validating 1/10: 100%|██████████| 19/19 [00:03<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.41m, loss_val=1.415, loss_train=1.430, acc_val=0.864, acc_train=0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 97/97 [00:21<00:00,  4.52it/s]\n",
      "Validating 2/10: 100%|██████████| 19/19 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.41m, loss_val=1.017, loss_train=0.178, acc_val=0.881, acc_train=0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 97/97 [00:21<00:00,  4.48it/s]\n",
      "Validating 3/10: 100%|██████████| 19/19 [00:02<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.41m, loss_val=0.888, loss_train=0.102, acc_val=0.880, acc_train=0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 97/97 [00:21<00:00,  4.58it/s]\n",
      "Validating 4/10: 100%|██████████| 19/19 [00:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.40m, loss_val=0.864, loss_train=0.059, acc_val=0.892, acc_train=0.985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 97/97 [00:21<00:00,  4.50it/s]\n",
      "Validating 5/10: 100%|██████████| 19/19 [00:03<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.41m, loss_val=0.880, loss_train=0.034, acc_val=0.887, acc_train=0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 97/97 [00:21<00:00,  4.51it/s]\n",
      "Validating 6/10: 100%|██████████| 19/19 [00:02<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.40m, loss_val=0.798, loss_train=0.021, acc_val=0.897, acc_train=0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 97/97 [00:20<00:00,  4.63it/s]\n",
      "Validating 7/10: 100%|██████████| 19/19 [00:02<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.40m, loss_val=0.796, loss_train=0.008, acc_val=0.902, acc_train=0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 97/97 [00:21<00:00,  4.56it/s]\n",
      "Validating 8/10: 100%|██████████| 19/19 [00:03<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.41m, loss_val=0.803, loss_train=0.003, acc_val=0.901, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 97/97 [00:21<00:00,  4.49it/s]\n",
      "Validating 9/10: 100%|██████████| 19/19 [00:03<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.41m, loss_val=0.800, loss_train=0.001, acc_val=0.902, acc_train=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 97/97 [00:21<00:00,  4.45it/s]\n",
      "Validating 10/10: 100%|██████████| 19/19 [00:02<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.41m, loss_val=0.800, loss_train=0.001, acc_val=0.903, acc_train=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, backbone, up_last=3, t=0):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(256, 10)\n",
    "        self.up_last = up_last\n",
    "        self.t = t\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x, self.t, up_last=self.up_last))\n",
    "\n",
    "\n",
    "NAME = 'cls_cifar10_uplast0'\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = Classifier(backbone, up_last=0).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR10 noised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1/10: 100%|██████████| 390/390 [00:33<00:00, 11.59it/s]\n",
      "Validating 1/10: 100%|██████████| 78/78 [00:04<00:00, 18.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.63m, loss_val=0.599, loss_train=0.194, acc_val=0.869, acc_train=0.960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 390/390 [00:32<00:00, 11.87it/s]\n",
      "Validating 2/10: 100%|██████████| 78/78 [00:04<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.62m, loss_val=0.616, loss_train=0.111, acc_val=0.855, acc_train=0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 390/390 [00:34<00:00, 11.33it/s]\n",
      "Validating 3/10: 100%|██████████| 78/78 [00:03<00:00, 23.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.63m, loss_val=0.562, loss_train=0.100, acc_val=0.865, acc_train=0.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 390/390 [00:32<00:00, 12.11it/s]\n",
      "Validating 4/10: 100%|██████████| 78/78 [00:04<00:00, 19.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.60m, loss_val=0.614, loss_train=0.092, acc_val=0.863, acc_train=0.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 390/390 [00:32<00:00, 12.10it/s]\n",
      "Validating 5/10: 100%|██████████| 78/78 [00:03<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.60m, loss_val=0.562, loss_train=0.073, acc_val=0.874, acc_train=0.978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 390/390 [00:32<00:00, 11.84it/s]\n",
      "Validating 6/10: 100%|██████████| 78/78 [00:02<00:00, 26.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.60m, loss_val=0.465, loss_train=0.039, acc_val=0.883, acc_train=0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 390/390 [00:33<00:00, 11.70it/s]\n",
      "Validating 7/10: 100%|██████████| 78/78 [00:03<00:00, 23.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.61m, loss_val=0.502, loss_train=0.021, acc_val=0.886, acc_train=0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 390/390 [00:31<00:00, 12.21it/s]\n",
      "Validating 8/10: 100%|██████████| 78/78 [00:03<00:00, 25.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.58m, loss_val=0.461, loss_train=0.010, acc_val=0.898, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 390/390 [00:33<00:00, 11.65it/s]\n",
      "Validating 9/10: 100%|██████████| 78/78 [00:03<00:00, 25.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.61m, loss_val=0.475, loss_train=0.005, acc_val=0.899, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 390/390 [00:33<00:00, 11.72it/s]\n",
      "Validating 10/10: 100%|██████████| 78/78 [00:03<00:00, 21.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.62m, loss_val=0.464, loss_train=0.003, acc_val=0.903, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(256, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=20, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'noised_cls_cifar10_uplast1'\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 11.10it/s]\n",
      "Training 1/10: 100%|██████████| 390/390 [00:32<00:00, 11.91it/s]\n",
      "Validating 1/10: 100%|██████████| 78/78 [00:04<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.62m, loss_val=0.539, loss_train=0.533, acc_val=0.842, acc_train=0.886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 390/390 [00:32<00:00, 12.03it/s]\n",
      "Validating 2/10: 100%|██████████| 78/78 [00:03<00:00, 21.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.60m, loss_val=0.537, loss_train=0.254, acc_val=0.843, acc_train=0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 390/390 [00:32<00:00, 12.09it/s]\n",
      "Validating 3/10: 100%|██████████| 78/78 [00:03<00:00, 22.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.59m, loss_val=0.530, loss_train=0.209, acc_val=0.851, acc_train=0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 390/390 [00:32<00:00, 11.93it/s]\n",
      "Validating 4/10: 100%|██████████| 78/78 [00:03<00:00, 23.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.60m, loss_val=0.449, loss_train=0.176, acc_val=0.872, acc_train=0.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 390/390 [00:32<00:00, 11.96it/s]\n",
      "Validating 5/10: 100%|██████████| 78/78 [00:03<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.61m, loss_val=0.452, loss_train=0.120, acc_val=0.871, acc_train=0.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 390/390 [00:33<00:00, 11.78it/s]\n",
      "Validating 6/10: 100%|██████████| 78/78 [00:03<00:00, 21.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.61m, loss_val=0.384, loss_train=0.072, acc_val=0.899, acc_train=0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 390/390 [00:32<00:00, 11.97it/s]\n",
      "Validating 7/10: 100%|██████████| 78/78 [00:03<00:00, 21.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.60m, loss_val=0.385, loss_train=0.043, acc_val=0.896, acc_train=0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 390/390 [00:34<00:00, 11.19it/s]\n",
      "Validating 8/10: 100%|██████████| 78/78 [00:03<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.64m, loss_val=0.377, loss_train=0.021, acc_val=0.906, acc_train=0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 390/390 [00:33<00:00, 11.76it/s]\n",
      "Validating 9/10: 100%|██████████| 78/78 [00:03<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.60m, loss_val=0.378, loss_train=0.009, acc_val=0.910, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 390/390 [00:32<00:00, 12.10it/s]\n",
      "Validating 10/10: 100%|██████████| 78/78 [00:03<00:00, 22.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.60m, loss_val=0.370, loss_train=0.005, acc_val=0.912, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(256, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=50, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'noised_cls_cifar10_ts50' #need to rerun \n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"DanielBairamian/ddpm-cifar10-32-ema\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  9.42it/s]\n",
      "Training 1/10: 100%|██████████| 390/390 [00:32<00:00, 11.92it/s]\n",
      "Validating 1/10: 100%|██████████| 78/78 [00:03<00:00, 19.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.61m, loss_val=0.539, loss_train=0.533, acc_val=0.842, acc_train=0.886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 390/390 [00:33<00:00, 11.52it/s]\n",
      "Validating 2/10: 100%|██████████| 78/78 [00:04<00:00, 17.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.64m, loss_val=0.537, loss_train=0.254, acc_val=0.843, acc_train=0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 390/390 [00:32<00:00, 11.84it/s]\n",
      "Validating 3/10: 100%|██████████| 78/78 [00:03<00:00, 22.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.61m, loss_val=0.530, loss_train=0.209, acc_val=0.851, acc_train=0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 390/390 [00:31<00:00, 12.19it/s]\n",
      "Validating 4/10: 100%|██████████| 78/78 [00:03<00:00, 21.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.60m, loss_val=0.449, loss_train=0.176, acc_val=0.872, acc_train=0.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 390/390 [00:33<00:00, 11.76it/s]\n",
      "Validating 5/10: 100%|██████████| 78/78 [00:03<00:00, 23.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.61m, loss_val=0.452, loss_train=0.120, acc_val=0.871, acc_train=0.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 390/390 [00:32<00:00, 11.96it/s]\n",
      "Validating 6/10: 100%|██████████| 78/78 [00:03<00:00, 23.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.60m, loss_val=0.384, loss_train=0.072, acc_val=0.899, acc_train=0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 390/390 [00:32<00:00, 12.00it/s]\n",
      "Validating 7/10: 100%|██████████| 78/78 [00:03<00:00, 21.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.60m, loss_val=0.385, loss_train=0.043, acc_val=0.896, acc_train=0.986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 390/390 [00:32<00:00, 11.90it/s]\n",
      "Validating 8/10: 100%|██████████| 78/78 [00:03<00:00, 23.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.60m, loss_val=0.377, loss_train=0.021, acc_val=0.906, acc_train=0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 390/390 [00:32<00:00, 11.91it/s]\n",
      "Validating 9/10: 100%|██████████| 78/78 [00:03<00:00, 20.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.61m, loss_val=0.378, loss_train=0.009, acc_val=0.910, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 390/390 [00:32<00:00, 12.14it/s]\n",
      "Validating 10/10: 100%|██████████| 78/78 [00:04<00:00, 16.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.61m, loss_val=0.370, loss_train=0.005, acc_val=0.912, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(256, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=20, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'noised_cls_cifar10_ts20'\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"DanielBairamian/ddpm-cifar10-32-ema\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 21.63it/s]\n",
      "Training 1/10: 100%|██████████| 390/390 [00:33<00:00, 11.68it/s]\n",
      "Validating 1/10: 100%|██████████| 78/78 [00:04<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.63m, loss_val=0.533, loss_train=0.606, acc_val=0.851, acc_train=0.881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 390/390 [00:33<00:00, 11.64it/s]\n",
      "Validating 2/10: 100%|██████████| 78/78 [00:03<00:00, 22.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.62m, loss_val=0.501, loss_train=0.231, acc_val=0.857, acc_train=0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 390/390 [00:32<00:00, 11.82it/s]\n",
      "Validating 3/10: 100%|██████████| 78/78 [00:03<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.61m, loss_val=0.525, loss_train=0.199, acc_val=0.849, acc_train=0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 390/390 [00:33<00:00, 11.51it/s]\n",
      "Validating 4/10: 100%|██████████| 78/78 [00:03<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.63m, loss_val=0.459, loss_train=0.169, acc_val=0.869, acc_train=0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 390/390 [00:33<00:00, 11.69it/s]\n",
      "Validating 5/10: 100%|██████████| 78/78 [00:03<00:00, 21.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.62m, loss_val=0.436, loss_train=0.116, acc_val=0.879, acc_train=0.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 390/390 [00:32<00:00, 11.84it/s]\n",
      "Validating 6/10: 100%|██████████| 78/78 [00:03<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.61m, loss_val=0.412, loss_train=0.072, acc_val=0.891, acc_train=0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 390/390 [00:32<00:00, 11.85it/s]\n",
      "Validating 7/10: 100%|██████████| 78/78 [00:03<00:00, 23.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.61m, loss_val=0.438, loss_train=0.040, acc_val=0.892, acc_train=0.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 390/390 [00:31<00:00, 12.21it/s]\n",
      "Validating 8/10: 100%|██████████| 78/78 [00:03<00:00, 20.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.59m, loss_val=0.392, loss_train=0.019, acc_val=0.904, acc_train=0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 390/390 [00:32<00:00, 12.14it/s]\n",
      "Validating 9/10: 100%|██████████| 78/78 [00:03<00:00, 21.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.60m, loss_val=0.404, loss_train=0.008, acc_val=0.905, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 390/390 [00:32<00:00, 11.97it/s]\n",
      "Validating 10/10: 100%|██████████| 78/78 [00:03<00:00, 23.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.60m, loss_val=0.398, loss_train=0.004, acc_val=0.907, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(256, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=20, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'noised_cls_cifar10_ts20_g'\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 22.43it/s]\n",
      "Training 1/10: 100%|██████████| 390/390 [00:32<00:00, 12.01it/s]\n",
      "Validating 1/10: 100%|██████████| 78/78 [00:03<00:00, 22.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.60m, loss_val=0.583, loss_train=0.594, acc_val=0.840, acc_train=0.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 390/390 [00:32<00:00, 12.05it/s]\n",
      "Validating 2/10: 100%|██████████| 78/78 [00:03<00:00, 22.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.60m, loss_val=0.537, loss_train=0.253, acc_val=0.842, acc_train=0.923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 390/390 [00:32<00:00, 11.84it/s]\n",
      "Validating 3/10: 100%|██████████| 78/78 [00:03<00:00, 23.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.61m, loss_val=0.492, loss_train=0.221, acc_val=0.859, acc_train=0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 390/390 [00:32<00:00, 12.04it/s]\n",
      "Validating 4/10: 100%|██████████| 78/78 [00:03<00:00, 21.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.60m, loss_val=0.526, loss_train=0.177, acc_val=0.859, acc_train=0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 390/390 [00:32<00:00, 11.85it/s]\n",
      "Validating 5/10: 100%|██████████| 78/78 [00:03<00:00, 21.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.61m, loss_val=0.412, loss_train=0.131, acc_val=0.876, acc_train=0.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 390/390 [00:32<00:00, 12.00it/s]\n",
      "Validating 6/10: 100%|██████████| 78/78 [00:03<00:00, 23.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.60m, loss_val=0.418, loss_train=0.080, acc_val=0.883, acc_train=0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 390/390 [00:32<00:00, 11.95it/s]\n",
      "Validating 7/10: 100%|██████████| 78/78 [00:03<00:00, 21.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.61m, loss_val=0.438, loss_train=0.043, acc_val=0.885, acc_train=0.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 390/390 [00:33<00:00, 11.72it/s]\n",
      "Validating 8/10: 100%|██████████| 78/78 [00:04<00:00, 18.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.63m, loss_val=0.439, loss_train=0.019, acc_val=0.899, acc_train=0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 390/390 [00:32<00:00, 12.12it/s]\n",
      "Validating 9/10: 100%|██████████| 78/78 [00:03<00:00, 23.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.59m, loss_val=0.419, loss_train=0.010, acc_val=0.903, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 390/390 [00:32<00:00, 11.87it/s]\n",
      "Validating 10/10: 100%|██████████| 78/78 [00:03<00:00, 21.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.61m, loss_val=0.423, loss_train=0.004, acc_val=0.906, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(256, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=50, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'noised_cls_cifar10_ts50_g'\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 22.81it/s]\n",
      "Training 1/10: 100%|██████████| 390/390 [00:31<00:00, 12.28it/s]\n",
      "Validating 1/10: 100%|██████████| 78/78 [00:03<00:00, 24.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.58m, loss_val=0.522, loss_train=0.609, acc_val=0.850, acc_train=0.881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 390/390 [00:32<00:00, 11.96it/s]\n",
      "Validating 2/10: 100%|██████████| 78/78 [00:03<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.60m, loss_val=0.603, loss_train=0.237, acc_val=0.846, acc_train=0.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 390/390 [00:33<00:00, 11.77it/s]\n",
      "Validating 3/10: 100%|██████████| 78/78 [00:03<00:00, 25.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.60m, loss_val=0.491, loss_train=0.210, acc_val=0.862, acc_train=0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 390/390 [00:33<00:00, 11.79it/s]\n",
      "Validating 4/10: 100%|██████████| 78/78 [00:04<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.62m, loss_val=0.477, loss_train=0.161, acc_val=0.866, acc_train=0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 390/390 [00:31<00:00, 12.19it/s]\n",
      "Validating 5/10: 100%|██████████| 78/78 [00:03<00:00, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.59m, loss_val=0.456, loss_train=0.118, acc_val=0.880, acc_train=0.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 390/390 [00:33<00:00, 11.73it/s]\n",
      "Validating 6/10: 100%|██████████| 78/78 [00:03<00:00, 22.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.61m, loss_val=0.425, loss_train=0.073, acc_val=0.885, acc_train=0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 390/390 [00:32<00:00, 11.95it/s]\n",
      "Validating 7/10: 100%|██████████| 78/78 [00:03<00:00, 23.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.60m, loss_val=0.408, loss_train=0.040, acc_val=0.898, acc_train=0.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 390/390 [00:31<00:00, 12.27it/s]\n",
      "Validating 8/10: 100%|██████████| 78/78 [00:03<00:00, 24.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.58m, loss_val=0.378, loss_train=0.021, acc_val=0.906, acc_train=0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 390/390 [00:32<00:00, 12.15it/s]\n",
      "Validating 9/10: 100%|██████████| 78/78 [00:03<00:00, 20.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.60m, loss_val=0.391, loss_train=0.008, acc_val=0.909, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 390/390 [00:32<00:00, 12.07it/s]\n",
      "Validating 10/10: 100%|██████████| 78/78 [00:03<00:00, 25.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.59m, loss_val=0.373, loss_train=0.005, acc_val=0.913, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(256, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=20, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'noised_cls_cifar10_ts20_g_adam'\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00,  4.13it/s]\n",
      "Training 1/10: 100%|██████████| 390/390 [00:32<00:00, 11.89it/s]\n",
      "Validating 1/10: 100%|██████████| 78/78 [00:03<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.61m, loss_val=0.526, loss_train=0.527, acc_val=0.847, acc_train=0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/10: 100%|██████████| 390/390 [00:32<00:00, 11.85it/s]\n",
      "Validating 2/10: 100%|██████████| 78/78 [00:03<00:00, 23.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.61m, loss_val=0.550, loss_train=0.253, acc_val=0.835, acc_train=0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/10: 100%|██████████| 390/390 [00:32<00:00, 12.09it/s]\n",
      "Validating 3/10: 100%|██████████| 78/78 [00:02<00:00, 26.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.59m, loss_val=0.526, loss_train=0.228, acc_val=0.850, acc_train=0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/10: 100%|██████████| 390/390 [00:31<00:00, 12.22it/s]\n",
      "Validating 4/10: 100%|██████████| 78/78 [00:03<00:00, 20.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.60m, loss_val=0.472, loss_train=0.165, acc_val=0.863, acc_train=0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/10: 100%|██████████| 390/390 [00:32<00:00, 11.87it/s]\n",
      "Validating 5/10: 100%|██████████| 78/78 [00:03<00:00, 24.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.60m, loss_val=0.441, loss_train=0.121, acc_val=0.875, acc_train=0.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/10: 100%|██████████| 390/390 [00:32<00:00, 11.97it/s]\n",
      "Validating 6/10: 100%|██████████| 78/78 [00:03<00:00, 25.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.60m, loss_val=0.430, loss_train=0.079, acc_val=0.888, acc_train=0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/10: 100%|██████████| 390/390 [00:31<00:00, 12.22it/s]\n",
      "Validating 7/10: 100%|██████████| 78/78 [00:03<00:00, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.60m, loss_val=0.365, loss_train=0.046, acc_val=0.902, acc_train=0.985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/10: 100%|██████████| 390/390 [00:33<00:00, 11.78it/s]\n",
      "Validating 8/10: 100%|██████████| 78/78 [00:03<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.62m, loss_val=0.398, loss_train=0.020, acc_val=0.905, acc_train=0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/10: 100%|██████████| 390/390 [00:32<00:00, 12.05it/s]\n",
      "Validating 9/10: 100%|██████████| 78/78 [00:03<00:00, 20.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.60m, loss_val=0.412, loss_train=0.009, acc_val=0.909, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/10: 100%|██████████| 390/390 [00:33<00:00, 11.69it/s]\n",
      "Validating 10/10: 100%|██████████| 78/78 [00:04<00:00, 17.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.63m, loss_val=0.395, loss_train=0.004, acc_val=0.912, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(256, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=20, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'ema_noised_cls_cifar10_ts20_adam'\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"DanielBairamian/ddpm-cifar10-32-ema\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 11.06it/s]\n",
      "Training 1/15: 100%|██████████| 390/390 [00:32<00:00, 12.04it/s]\n",
      "Validating 1/15: 100%|██████████| 78/78 [00:03<00:00, 22.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.60m, loss_val=0.605, loss_train=0.507, acc_val=0.826, acc_train=0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2/15: 100%|██████████| 390/390 [00:33<00:00, 11.51it/s]\n",
      "Validating 2/15: 100%|██████████| 78/78 [00:03<00:00, 21.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 in 0.63m, loss_val=0.673, loss_train=0.275, acc_val=0.810, acc_train=0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3/15: 100%|██████████| 390/390 [00:33<00:00, 11.75it/s]\n",
      "Validating 3/15: 100%|██████████| 78/78 [00:03<00:00, 22.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 in 0.61m, loss_val=0.526, loss_train=0.246, acc_val=0.850, acc_train=0.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4/15: 100%|██████████| 390/390 [00:33<00:00, 11.63it/s]\n",
      "Validating 4/15: 100%|██████████| 78/78 [00:03<00:00, 23.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 in 0.61m, loss_val=0.508, loss_train=0.205, acc_val=0.852, acc_train=0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5/15: 100%|██████████| 390/390 [00:34<00:00, 11.41it/s]\n",
      "Validating 5/15: 100%|██████████| 78/78 [00:02<00:00, 26.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 in 0.62m, loss_val=0.499, loss_train=0.172, acc_val=0.861, acc_train=0.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6/15: 100%|██████████| 390/390 [00:32<00:00, 12.12it/s]\n",
      "Validating 6/15: 100%|██████████| 78/78 [00:02<00:00, 26.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 in 0.59m, loss_val=0.420, loss_train=0.146, acc_val=0.873, acc_train=0.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7/15: 100%|██████████| 390/390 [00:31<00:00, 12.37it/s]\n",
      "Validating 7/15: 100%|██████████| 78/78 [00:03<00:00, 22.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 in 0.58m, loss_val=0.446, loss_train=0.120, acc_val=0.878, acc_train=0.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8/15: 100%|██████████| 390/390 [00:32<00:00, 11.82it/s]\n",
      "Validating 8/15: 100%|██████████| 78/78 [00:03<00:00, 23.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 in 0.61m, loss_val=0.440, loss_train=0.085, acc_val=0.888, acc_train=0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9/15: 100%|██████████| 390/390 [00:31<00:00, 12.22it/s]\n",
      "Validating 9/15: 100%|██████████| 78/78 [00:02<00:00, 28.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 in 0.58m, loss_val=0.431, loss_train=0.057, acc_val=0.888, acc_train=0.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10/15: 100%|██████████| 390/390 [00:32<00:00, 12.07it/s]\n",
      "Validating 10/15: 100%|██████████| 78/78 [00:03<00:00, 22.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 in 0.60m, loss_val=0.423, loss_train=0.034, acc_val=0.895, acc_train=0.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 11/15: 100%|██████████| 390/390 [00:31<00:00, 12.26it/s]\n",
      "Validating 11/15: 100%|██████████| 78/78 [00:03<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=11 in 0.59m, loss_val=0.428, loss_train=0.019, acc_val=0.905, acc_train=0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 12/15: 100%|██████████| 390/390 [00:32<00:00, 12.08it/s]\n",
      "Validating 12/15: 100%|██████████| 78/78 [00:03<00:00, 21.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=12 in 0.60m, loss_val=0.449, loss_train=0.010, acc_val=0.906, acc_train=0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 13/15: 100%|██████████| 390/390 [00:31<00:00, 12.19it/s]\n",
      "Validating 13/15: 100%|██████████| 78/78 [00:03<00:00, 22.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=13 in 0.59m, loss_val=0.440, loss_train=0.005, acc_val=0.909, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 14/15: 100%|██████████| 390/390 [00:33<00:00, 11.50it/s]\n",
      "Validating 14/15: 100%|██████████| 78/78 [00:04<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=14 in 0.64m, loss_val=0.457, loss_train=0.002, acc_val=0.911, acc_train=0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 15/15: 100%|██████████| 390/390 [00:32<00:00, 12.10it/s]\n",
      "Validating 15/15: 100%|██████████| 78/78 [00:04<00:00, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=15 in 0.60m, loss_val=0.456, loss_train=0.001, acc_val=0.911, acc_train=1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(256, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=50, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'ema_noised_cls_cifar10_ts50_ep15'\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 15\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"DanielBairamian/ddpm-cifar10-32-ema\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]An error occurred while trying to fetch /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/jovyan/.cache/huggingface/hub/models--DanielBairamian--ddpm-cifar10-32-ema/snapshots/d9caa7c75cd561fed983fa71979309a8435b20f5/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 2/2 [00:00<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 in 0.61m, loss_val=0.568, loss_train=0.533, acc_val=0.840, acc_train=0.888\n",
      "epoch=2 in 0.60m, loss_val=0.543, loss_train=0.267, acc_val=0.842, acc_train=0.920\n",
      "epoch=3 in 0.58m, loss_val=0.482, loss_train=0.223, acc_val=0.863, acc_train=0.931\n",
      "epoch=4 in 0.60m, loss_val=0.497, loss_train=0.165, acc_val=0.874, acc_train=0.948\n",
      "epoch=5 in 0.60m, loss_val=0.421, loss_train=0.125, acc_val=0.882, acc_train=0.961\n",
      "epoch=6 in 0.60m, loss_val=0.388, loss_train=0.078, acc_val=0.889, acc_train=0.974\n",
      "epoch=7 in 0.59m, loss_val=0.404, loss_train=0.042, acc_val=0.895, acc_train=0.986\n",
      "epoch=8 in 0.60m, loss_val=0.375, loss_train=0.020, acc_val=0.910, acc_train=0.994\n",
      "epoch=9 in 0.59m, loss_val=0.383, loss_train=0.008, acc_val=0.916, acc_train=0.998\n",
      "epoch=10 in 0.59m, loss_val=0.386, loss_train=0.004, acc_val=0.916, acc_train=0.999\n"
     ]
    }
   ],
   "source": [
    "class ClassifierNoised(nn.Module):\n",
    "    def __init__(self, backbone, scheduler, up_last=3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.scheduler = scheduler\n",
    "        self.head = nn.Linear(256, 100)\n",
    "        self.up_last = up_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t = torch.randint(low=0, high=10, size=(x.shape[0],), device=x.device, dtype=torch.int64)\n",
    "        noise = torch.randn(x.shape, device=x.device)\n",
    "        noised = self.scheduler.add_noise(x, noise, t)\n",
    "        out = self.backbone(noised, t, up_last=self.up_last)\n",
    "        return self.head(out)\n",
    "\n",
    "NAME = 'ema_noised_cls_cifar10_ts10'\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "set_deterministic()\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar10', horizontal_flip=True)\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"DanielBairamian/ddpm-cifar10-32-ema\")\n",
    "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
    "backbone = DiffusionEncoder(hf_unet)\n",
    "model = ClassifierNoised(backbone, hf_scheduler, up_last=1).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_test = tt.Compose([tt.ToTensor(), tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225])])\n",
    "transform_train = tt.Compose([tt.ToTensor(), tt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225]), tt.RandomHorizontalFlip()])\n",
    "\n",
    "dataset_train = torchvision.datasets.CIFAR10(root='./datasets/cifar10', train=True, download=True, transform=transform_train)\n",
    "dataset_test = torchvision.datasets.CIFAR10(root='./datasets/cifar10', train=False, download=True, transform=transform_test)\n",
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n",
    "test_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = torchvision.datasets.CelebA(root='./datasets/celeba', split='train', download=True)\n",
    "# dataset_test = torchvision.datasets.CelebA(root='./datasets/celeba', split='val', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mocov2(model, pretrained):\n",
    "    checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "    for k in list(state_dict.keys()):\n",
    "        if k.startswith(\"module.encoder_q\") and not k.startswith(\n",
    "            \"module.encoder_q.fc\"\n",
    "        ):\n",
    "            state_dict[k[len(\"module.encoder_q.\") :]] = state_dict[k]\n",
    "        del state_dict[k]\n",
    "\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    assert set(msg.missing_keys) == {\"fc.weight\", \"fc.bias\"}\n",
    "    print(\"=> loaded pre-trained model '{}'\".format(pretrained))\n",
    "    model.fc=nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1699311/2867813472.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded pre-trained model './moco_v2_800ep_pretrain.pth.tar'\n",
      "epoch=1 in 0.39m, loss_val=1.162, loss_train=1.282, acc_val=0.599, acc_train=0.533\n",
      "epoch=2 in 0.38m, loss_val=0.863, loss_train=0.844, acc_val=0.699, acc_train=0.704\n",
      "epoch=3 in 0.38m, loss_val=0.768, loss_train=0.690, acc_val=0.736, acc_train=0.761\n",
      "epoch=4 in 0.36m, loss_val=0.638, loss_train=0.586, acc_val=0.782, acc_train=0.797\n",
      "epoch=5 in 0.35m, loss_val=0.605, loss_train=0.489, acc_val=0.798, acc_train=0.833\n",
      "epoch=6 in 0.36m, loss_val=0.574, loss_train=0.401, acc_val=0.804, acc_train=0.863\n",
      "epoch=7 in 0.36m, loss_val=0.523, loss_train=0.317, acc_val=0.828, acc_train=0.893\n",
      "epoch=8 in 0.36m, loss_val=0.515, loss_train=0.237, acc_val=0.839, acc_train=0.920\n",
      "epoch=9 in 0.36m, loss_val=0.526, loss_train=0.175, acc_val=0.843, acc_train=0.944\n",
      "epoch=10 in 0.34m, loss_val=0.531, loss_train=0.135, acc_val=0.846, acc_train=0.958\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(2048, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "\n",
    "NAME = 'cls_mocov2'\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "backbone = torchvision.models.resnet50(weights=None)\n",
    "load_mocov2(backbone, './moco_v2_800ep_pretrain.pth.tar')\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.maxpool = nn.Identity()\n",
    "model = Classifier(backbone).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized with trainable params: 23,508,032 || total params: 23,508,032 || trainable%: 100.000\n"
     ]
    }
   ],
   "source": [
    "print_parameters(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1930939/2867813472.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded pre-trained model './moco_v2_800ep_pretrain.pth.tar'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mmaxpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[0;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     25\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR)\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(2048, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "\n",
    "NAME = 'cls_mocov2'\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "backbone = torchvision.models.resnet50(weights=None)\n",
    "load_mocov2(backbone, './moco_v2_800ep_pretrain.pth.tar')\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.maxpool = nn.Identity()\n",
    "model = Classifier(backbone).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1930939/2867813472.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pretrained, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded pre-trained model './moco_v2_800ep_pretrain.pth.tar'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m scaler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mGradScaler()\n\u001b[1;32m     30\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tensorboard/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrain_fp16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFREQ_SAVE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./checkpoints/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m, in \u001b[0;36mtrain_fp16\u001b[0;34m(writer, model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, freq_save, save_path, scaler, device)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     53\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 54\u001b[0m     metrics_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fp16_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     metrics_val \u001b[38;5;241m=\u001b[39m test_fp16(\n\u001b[1;32m     59\u001b[0m         model, criterion, val_loader,\n\u001b[1;32m     60\u001b[0m         tqdm_desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mtrain_fp16_epoch\u001b[0;34m(model, optimizer, criterion, scheduler, data_loader, tqdm_desc, scaler, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, target)\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     38\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/gm_env/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(2048, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "\n",
    "NAME = 'cls_mocov2_cifar100'\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "TRAIN_EPOCH = 10\n",
    "FREQ_SAVE = 100\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "train_loader, test_loader = get_loaders(BATCH_SIZE, dataset='cifar100', horizontal_flip=True)\n",
    "\n",
    "backbone = torchvision.models.resnet50(weights=None)\n",
    "load_mocov2(backbone, './moco_v2_800ep_pretrain.pth.tar')\n",
    "model = Classifier(backbone).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
    "\n",
    "train_fp16(writer, model, optimizer, scheduler, criterion, \n",
    "    train_loader, test_loader, TRAIN_EPOCH, FREQ_SAVE,\n",
    "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
