{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THr1HhI1kyef"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as tt\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from diffusers import DDPMPipeline\n",
        "# UTILS\n",
        "def set_deterministic(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def readable_number(num):\n",
        "    num_str = str(num)[::-1]\n",
        "    res = ''\n",
        "    i_prev = 0\n",
        "    for i in range(3, len(num_str), 3):\n",
        "        res += num_str[i_prev:i] + ','\n",
        "        i_prev = i\n",
        "    if i_prev < len(num_str):\n",
        "        res += num_str[i_prev:]\n",
        "    return res[::-1]\n",
        "\n",
        "def log(writer, metrics, epoch):\n",
        "    writer.add_scalars('loss', {'train': metrics['loss_train'], 'test': metrics['loss_test']}, epoch)\n",
        "    writer.add_scalars('accuracy', {'train': metrics['accuracy_train'], 'test': metrics['accuracy_test']}, epoch)\n",
        "    writer.flush()\n",
        "\n",
        "def save_checkpoint(state, path, epoch, test_loss):\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "    torch.save(state, f'{path}/{epoch}_valloss={test_loss:.3f}.pt')\n",
        "\n",
        "def get_parameters(model):\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    return total, trainable\n",
        "\n",
        "def print_parameters(model):\n",
        "    total, trainable = get_parameters(model)\n",
        "    print(f'model initialized with trainable params: {readable_number(trainable)} || total params: {readable_number(total)} || trainable%: {trainable/total * 100:.3f}')\n",
        "\n",
        "def test_fp16(model, criterion, data_loader, tqdm_desc, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred, test_loss = [], [], []\n",
        "    # for imgs, target in data_loader:\n",
        "    for imgs, target in tqdm(data_loader, desc=tqdm_desc):\n",
        "        imgs, target = imgs.to(device), target.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                logits = model(imgs)\n",
        "                loss = criterion(logits, target)\n",
        "\n",
        "        test_loss.append(loss.item())\n",
        "        y_pred.extend(logits.argmax(dim=1).flatten().tolist())\n",
        "        y_true.extend(target.flatten().tolist())\n",
        "\n",
        "    y_true, y_pred, test_loss = np.array(y_true), np.array(y_pred), np.array(test_loss)\n",
        "    metrics = {}\n",
        "    metrics['accuracy_test'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['loss_test'] = np.mean(test_loss)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def train_fp16_epoch(model, optimizer, criterion, scheduler, data_loader, tqdm_desc, scaler, device):\n",
        "    model.train()\n",
        "    y_true, y_pred, train_loss = [], [], []\n",
        "    #for imgs, target in data_loader:\n",
        "    for imgs, target in tqdm(data_loader, desc=tqdm_desc):\n",
        "        imgs, target = imgs.to(device), target.to(device)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "        y_pred.extend(logits.argmax(dim=1).flatten().tolist())\n",
        "        y_true.extend(target.flatten().tolist())\n",
        "\n",
        "    y_true, y_pred, train_loss = np.array(y_true), np.array(y_pred), np.array(train_loss)\n",
        "    metrics = {}\n",
        "    metrics['accuracy_train'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['loss_train'] = np.mean(train_loss)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def train_fp16(writer, model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, freq_save, save_path, scaler, device):\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        start = time.time()\n",
        "        metrics_train = train_fp16_epoch(\n",
        "            model, optimizer, criterion, scheduler, train_loader,\n",
        "            tqdm_desc=f'Training {epoch}/{num_epochs}', scaler=scaler, device=device\n",
        "        )\n",
        "        metrics_val = test_fp16(\n",
        "            model, criterion, val_loader,\n",
        "            tqdm_desc=f'Validating {epoch}/{num_epochs}', device=device\n",
        "        )\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        if epoch % freq_save == 0:\n",
        "            save_checkpoint(model.state_dict(), save_path, epoch, metrics_val[\"loss_test\"])\n",
        "\n",
        "        log(writer, {**metrics_val, **metrics_train}, epoch)\n",
        "        end = time.time()\n",
        "        print(f'{epoch=} in {((end - start) / 60):.2f}m, loss_val={metrics_val[\"loss_test\"]:.3f}, loss_train={metrics_train[\"loss_train\"]:.3f}, acc_val={metrics_val[\"accuracy_test\"]:.3f}, acc_train={metrics_train[\"accuracy_train\"]:.3f}')\n",
        "# SimCLR\n",
        "## Model\n",
        "# !pip install lightly\n",
        "from lightly.loss import NTXentLoss\n",
        "from lightly.models.modules import SimCLRProjectionHead\n",
        "from lightly.transforms.simclr_transform import SimCLRTransform\n",
        "from lightly.data import LightlyDataset\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        num_feats = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.projection_head = SimCLRProjectionHead(num_feats, 512, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(x)\n",
        "        return z\n",
        "def train_simclr_backbone(backbone, dataset, device, num_epochs=10):\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    drop_last=True)\n",
        "\n",
        "    criterion = NTXentLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.06)\n",
        "\n",
        "    model = SimCLR(backbone)\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(\"Starting Training\")\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
        "            x0, x1 = batch[0]\n",
        "            x0 = x0.to(device)\n",
        "            x1 = x1.to(device)\n",
        "            z0 = model(x0)\n",
        "            z1 = model(x1)\n",
        "            loss = criterion(z0, z1)\n",
        "            total_loss += loss.detach()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
        "\n",
        "    return model\n",
        "def get_backbone_pretrained():\n",
        "  backbone = torchvision.models.resnet50()\n",
        "\n",
        "  checkpoint_url = \"https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_simclr_2023-06-22_09-11-13/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt\"\n",
        "\n",
        "  state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location='cpu', weights_only=True)\n",
        "\n",
        "  backbone_state_dict = {\n",
        "      k.replace(\"backbone.\", \"\"): v\n",
        "      for k, v in state_dict[\"state_dict\"].items()\n",
        "      if k.startswith(\"backbone.\")\n",
        "  }\n",
        "\n",
        "  backbone.fc = nn.Identity()\n",
        "\n",
        "  backbone.load_state_dict(backbone_state_dict, strict=False)\n",
        "\n",
        "  return backbone\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "t = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize()\n",
        "    ])\n",
        "cifar10_dataset = torchvision.datasets.CIFAR10(\"datasets/cifar10/train\", train=True, download=True, transform=t)\n",
        "cifar10_loader = torch.utils.data.DataLoader(\n",
        "    cifar10_dataset,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=1\n",
        ")\n",
        "cifar10_dataset_test = torchvision.datasets.CIFAR10(\"datasets/cifar10/test\", train=False, download=True, transform=t)\n",
        "cifar10_loader_test = torch.utils.data.DataLoader(\n",
        "    cifar10_dataset,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=1\n",
        ")\n",
        "## Data\n",
        "import glob\n",
        "import json\n",
        "\n",
        "\n",
        "PATH = './CLEVR_v1.0/'\n",
        "class CLEVRNumObjectsDataset(Dataset):\n",
        "    def __init__(self, transform, path, train=True):\n",
        "        super().__init__()\n",
        "        path = f'{PATH}images/train/*.png' if train else f'{PATH}images/val/*.png'\n",
        "\n",
        "        self.preprocessor = transform\n",
        "\n",
        "        self.data = glob.glob(path)\n",
        "        self.data.sort()\n",
        "        labels_path = f'{PATH}scenes/CLEVR_train_scenes.json' if train else \\\n",
        "            f'{PATH}scenes/CLEVR_val_scenes.json'\n",
        "        with open(labels_path) as f:\n",
        "            scene_data = json.load(f)\n",
        "\n",
        "        self.labels = torch.LongTensor([len(s['objects']) for s in scene_data['scenes']][:len(self.data)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        return self.preprocessor(Image.open(self.data[idx]).convert('RGB')), self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def num_classes(self):\n",
        "        return int(max(self.labels) + 1)\n",
        "transform = SimCLRTransform(input_size=256)\n",
        "contrastive_train_dataset = LightlyDataset(f\"{PATH}images/train\", transform=transform)\n",
        "numobj_dataset = CLEVRNumObjectsDataset(t, PATH)\n",
        "numobj_loader = torch.utils.data.DataLoader(\n",
        "    numobj_dataset,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=1\n",
        ")\n",
        "\n",
        "numobj_dataset_test = CLEVRNumObjectsDataset(t, PATH, train=False)\n",
        "numobj_loader_test = torch.utils.data.DataLoader(\n",
        "    numobj_dataset_test,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=1\n",
        ")\n",
        "# Training\n",
        "device = torch.device('cuda')\n",
        "class ObjectCount(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = nn.Linear(2048, 11)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.head(self.backbone(x))\n",
        "def train_test_pretrained(train_loader, test_loader, NAME, backbone_frozen =True, num_epoch=10, FREQ_SAVE=100):\n",
        "    backbone = get_backbone_pretrained()\n",
        "    model = ObjectCount(backbone).to(device)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    if backbone_frozen:\n",
        "        optimizer = torch.optim.Adam(model.head.parameters(), lr=0.001)\n",
        "    else:\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epoch)\n",
        "\n",
        "    scaler = torch.amp.GradScaler()\n",
        "    writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
        "\n",
        "    train_fp16(writer, model, optimizer, scheduler, criterion,\n",
        "        train_loader, test_loader, num_epoch, FREQ_SAVE,\n",
        "        save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=device)\n",
        "## SimCLR on CIFAR10\n",
        "train_test_pretrained(cifar10_loader, cifar10_loader_test, 'simclr_cifar10_bb_frozen')\n",
        "train_test_pretrained(cifar10_loader, cifar10_loader_test, 'simclr_cifar10_bb_not_frozen', backbone_frozen=False)\n",
        "backbone = get_backbone_pretrained()\n",
        "cifar10_contrasrtive_dataset = LightlyDataset(f\"{PATH}images/train\", transform=transform)\n",
        "backbone = train_simclr_backbone(backbone, cifar10_contrasrtive_dataset, device)\n",
        "train_test_pretrained(numobj_loader, numobj_loader_test, 'simclr_cifar10_finetuned')\n",
        "## SimCLR on CLEVR\n",
        "train_test_pretrained(numobj_loader, numobj_loader_test, 'simclr_clvr_bb_frozen')\n",
        "\"\"\"\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=tensorboard/simclr_clvr_bb_frozen\n",
        "\"\"\"\n",
        "train_test_pretrained(numobj_loader, numobj_loader_test, 'simclr_clvr_bb_not_frozen', backbone_frozen=False)\n",
        "\"\"\"\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=tensorboard/simclr_clvr_bb_not_frozen\n",
        "\"\"\"\n",
        "backbone = get_backbone_pretrained()\n",
        "clevr_contrastive_dataset = LightlyDataset(f\"{PATH}images/train\", transform=transform)\n",
        "backbone = train_simclr_backbone(backbone, clevr_contrastive_dataset, device)\n",
        "train_test_pretrained(numobj_loader, numobj_loader_test, 'simclr_clvr_finetuned')\n",
        "\"\"\"\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=tensorboard/simclr_clvr_finetuned\n",
        "\"\"\"\n",
        "## Diffusion Encoder on CLEVR\n",
        "pipe = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\")\n",
        "hf_unet, hf_scheduler = pipe.unet, pipe.scheduler\n",
        "class DiffusionEncoder(nn.Module):\n",
        "    def __init__(self, unet):\n",
        "        super().__init__()\n",
        "        self.unet = unet\n",
        "\n",
        "    def forward(self, imgs, timestep, class_labels=None, up_last=-1, GAP=True):\n",
        "        params = 0\n",
        "        # 0. center input if necessary\n",
        "        if self.unet.config.center_input_sample:\n",
        "            imgs = 2 * imgs - 1.0\n",
        "\n",
        "        # 1. time\n",
        "        timesteps = timestep\n",
        "        if not torch.is_tensor(timesteps):\n",
        "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=imgs.device)\n",
        "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
        "            timesteps = timesteps[None].to(imgs.device)\n",
        "\n",
        "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        timesteps = timesteps * torch.ones(imgs.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
        "\n",
        "        t_emb = self.unet.time_proj(timesteps)\n",
        "\n",
        "        # timesteps does not contain any weights and will always return f32 tensors\n",
        "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
        "        # there might be better ways to encapsulate this.\n",
        "        t_emb = t_emb.to(dtype=self.unet.dtype)\n",
        "        emb = self.unet.time_embedding(t_emb)\n",
        "\n",
        "        total = get_parameters(self.unet.time_embedding)[0]\n",
        "        params += total\n",
        "        # print(f'time_embedding {total}')\n",
        "\n",
        "        if self.unet.class_embedding is not None:\n",
        "            if class_labels is None:\n",
        "                raise ValueError(\"class_labels should be provided when doing class conditioning\")\n",
        "\n",
        "            if self.unet.config.class_embed_type == \"timestep\":\n",
        "                class_labels = self.unet.time_proj(class_labels)\n",
        "\n",
        "            class_emb = self.unet.class_embedding(class_labels).to(dtype=self.unet.dtype)\n",
        "            emb = emb + class_emb\n",
        "\n",
        "            total = get_parameters(self.unet.class_embedding)[0]\n",
        "            params += total\n",
        "            # print(f'time_embedding {total}')\n",
        "        elif self.unet.class_embedding is None and class_labels is not None:\n",
        "            raise ValueError(\"class_embedding needs to be initialized in order to use class conditioning\")\n",
        "\n",
        "        # 2. pre-process\n",
        "        skip_sample = imgs\n",
        "        imgs = self.unet.conv_in(imgs)\n",
        "\n",
        "        total = get_parameters(self.unet.conv_in)[0]\n",
        "        params += total\n",
        "        # print(f'conv_in {total}')\n",
        "\n",
        "\n",
        "        # 3. down\n",
        "        down_block_res_samples = (imgs,)\n",
        "        for downsample_block in self.unet.down_blocks:\n",
        "            if hasattr(downsample_block, \"skip_conv\"):\n",
        "                imgs, res_samples, skip_sample = downsample_block(\n",
        "                    hidden_states=imgs, temb=emb, skip_sample=skip_sample\n",
        "                )\n",
        "            else:\n",
        "                imgs, res_samples = downsample_block(hidden_states=imgs, temb=emb)\n",
        "\n",
        "            down_block_res_samples += res_samples\n",
        "\n",
        "            total = get_parameters(downsample_block)[0]\n",
        "            params += total\n",
        "            # print(f'downsample_block {total}')\n",
        "\n",
        "        # 4. mid\n",
        "        imgs = self.unet.mid_block(imgs, emb)\n",
        "        # print(f'midlle, {imgs.shape=}')\n",
        "\n",
        "        total = get_parameters(self.unet.mid_block)[0]\n",
        "        params += total\n",
        "        # print(f'mid_block {total}')\n",
        "\n",
        "        # 5. up\n",
        "        skip_sample = None\n",
        "        for i, upsample_block in enumerate(self.unet.up_blocks):\n",
        "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
        "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
        "\n",
        "            if hasattr(upsample_block, \"skip_conv\"):\n",
        "                imgs, skip_sample = upsample_block(imgs, res_samples, emb, skip_sample)\n",
        "            else:\n",
        "                imgs = upsample_block(imgs, res_samples, emb)\n",
        "\n",
        "            total = get_parameters(upsample_block)[0]\n",
        "            params += total\n",
        "            # print(f'upsample_block {total}')\n",
        "\n",
        "            if up_last == i:\n",
        "                # print(f'params used = {readable_number(params)}')\n",
        "                return imgs.mean(dim=[2, 3]) if GAP else imgs\n",
        "\n",
        "\n",
        "        # 6. post-process\n",
        "        imgs = self.unet.conv_norm_out(imgs)\n",
        "        imgs = self.unet.conv_act(imgs)\n",
        "        imgs = self.unet.conv_out(imgs)\n",
        "\n",
        "        if skip_sample is not None:\n",
        "            imgs += skip_sample\n",
        "\n",
        "        if self.unet.config.time_embedding_type == \"fourier\":\n",
        "            timesteps = timesteps.reshape((imgs.shape[0], *([1] * len(imgs.shape[1:]))))\n",
        "            imgs = imgs / timesteps\n",
        "\n",
        "        return imgs\n",
        "torch.cuda.empty_cache()\n",
        "class ObjectCountDiffusion(nn.Module):\n",
        "    def __init__(self, backbone, up_last=1, t=0):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "        if up_last == 1:\n",
        "          #self.lin = nn.Linear(512, 2048)\n",
        "          self.dim = 512\n",
        "        elif up_last == 2:\n",
        "          #self.lin = nn.Linear(256, 2048)\n",
        "          self.dim = 256\n",
        "\n",
        "        self.head = nn.Linear(self.dim, 11)\n",
        "\n",
        "        self.up_last = up_last\n",
        "        self.t = t\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.head(self.backbone(x, self.t, up_last=self.up_last))\n",
        "backbone = DiffusionEncoder(hf_unet)\n",
        "model = ObjectCountDiffusion(backbone, up_last=1)\n",
        "model = model.to(device)\n",
        "\n",
        "NAME = 'obj_count_diffusion'\n",
        "LR = 1e-3\n",
        "TRAIN_EPOCH = 10\n",
        "FREQ_SAVE = 100\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
        "\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "writer = SummaryWriter(f'./tensorboard/{NAME}')\n",
        "\n",
        "train_fp16(writer, model, optimizer, scheduler, criterion,\n",
        "    numobj_loader, numobj_loader_test, TRAIN_EPOCH, FREQ_SAVE,\n",
        "    save_path=f'./checkpoints/{NAME}/', scaler=scaler, device=device)\n",
        "\"\"\"\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=tensorboard/obj_count_diffusion\n",
        "\"\"\"\n",
        "P.S: К сожалению я делал все в разных файлах и не смог перенести output, а обучать все заново не было возможности. Этот ноутбук - компиляция моего финального кода для проверки, результаты указал в презентации и отчете. Извиняюсь и понимаю, что из-за этого оценка может быть ниже, однако надеюсь на понимание.\n",
        "Спасибо!\n",
        "train_test_pretrained(numobj_loader, numobj_loader_test, 'simclr_clvr_bb_frozen')\n",
        "\"\"\"\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=tensorboard/simclr_clvr_bb_frozen\n",
        "\"\"\"\n",
        "train_test_pretrained(numobj_loader, numobj_loader_test, 'simclr_clvr_bb_not_frozen', backbone_frozen=False)\n",
        "\"\"\"\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=tensorboard/simclr_clvr_bb_not_frozen\n",
        "\"\"\"\n",
        "backbone = get_backbone_pretrained()\n",
        "clevr_contrastive_dataset = LightlyDataset(f\"{PATH}images/train\", transform=transform)\n",
        "backbone = train_simclr_backbone(backbone, clevr_contrastive_dataset, device)\n",
        "train_test_pretrained(numobj_loader, numobj_loader_test, 'simclr_clvr_finetuned')\n",
        "\"\"\"\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=tensorboard/simclr_clvr_finetuned\n",
        "\"\"\""
      ]
    }
  ]
}